{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0101 15:01:38.928117 140031185740032 809976296.py:55] Using cuda:7 for training.\n",
      "I0101 15:01:39.293020 140031185740032 809976296.py:76] Dataset Coauthor, Data(x=[18333, 6805], edge_index=[2, 163788], y=[18333]).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680eb233e2fc406387523ec259600180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/GengHao/Anaconda/lib/python3.9/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Epoch: 250, Test Acc: 0.9129\n",
      "[Test] Epoch: 250, Test Acc: 0.9172\n",
      "[Test] Epoch: 250, Test Acc: 0.9195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/GengHao/Anaconda/lib/python3.9/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Epoch: 500, Test Acc: 0.9167\n",
      "[Test] Epoch: 500, Test Acc: 0.9243\n",
      "[Test] Epoch: 500, Test Acc: 0.9197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/GengHao/Anaconda/lib/python3.9/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Epoch: 750, Test Acc: 0.9235\n",
      "[Test] Epoch: 750, Test Acc: 0.9257\n",
      "[Test] Epoch: 750, Test Acc: 0.9255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/GengHao/Anaconda/lib/python3.9/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Epoch: 1000, Test Acc: 0.9271\n",
      "[Test] Epoch: 1000, Test Acc: 0.9302\n",
      "[Test] Epoch: 1000, Test Acc: 0.9280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/GengHao/Anaconda/lib/python3.9/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Epoch: 1250, Test Acc: 0.9305\n",
      "[Test] Epoch: 1250, Test Acc: 0.9313\n",
      "[Test] Epoch: 1250, Test Acc: 0.9296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/GengHao/Anaconda/lib/python3.9/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31170/809976296.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PyTorch version: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/GengHao/Anaconda/lib/python3.9/site-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    306\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/GengHao/Anaconda/lib/python3.9/site-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31170/809976296.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_epochs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31170/809976296.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(step)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# update online network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/GengHao/Anaconda/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/GengHao/Anaconda/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import logging\n",
    "import os\n",
    "import sys \n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from bgrl import *\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('model_seed', None, 'Random seed used for model initialization and training.')\n",
    "flags.DEFINE_integer('data_seed', 1, 'Random seed used to generate train/val/test split.')\n",
    "flags.DEFINE_integer('num_eval_splits', 3, 'Number of different train/test splits the model will be evaluated over.')\n",
    "\n",
    "# Dataset.\n",
    "flags.DEFINE_enum('dataset', 'coauthor-cs',\n",
    "                  ['amazon-computers', 'amazon-photos', 'coauthor-cs', 'coauthor-physics', 'wiki-cs'],\n",
    "                  'Which graph dataset to use.')\n",
    "flags.DEFINE_string('dataset_dir', './data', 'Where the dataset resides.')\n",
    "\n",
    "# Architecture.\n",
    "flags.DEFINE_multi_integer('graph_encoder_layer', None, 'Conv layer sizes.')\n",
    "flags.DEFINE_integer('predictor_hidden_size', 512, 'Hidden size of projector.')\n",
    "\n",
    "# Training hyperparameters.\n",
    "flags.DEFINE_integer('epochs', 10000, 'The number of training epochs.')\n",
    "flags.DEFINE_float('lr', 1e-5, 'The learning rate for model training.')\n",
    "flags.DEFINE_float('weight_decay', 1e-5, 'The value of the weight decay for training.')\n",
    "flags.DEFINE_float('mm', 0.99, 'The momentum for moving average.')\n",
    "flags.DEFINE_integer('lr_warmup_epochs', 1000, 'Warmup period for learning rate.')\n",
    "\n",
    "# Augmentations.\n",
    "flags.DEFINE_float('drop_edge_p_1', 0., 'Probability of edge dropout 1.')\n",
    "flags.DEFINE_float('drop_feat_p_1', 0., 'Probability of node feature dropout 1.')\n",
    "flags.DEFINE_float('drop_edge_p_2', 0., 'Probability of edge dropout 2.')\n",
    "flags.DEFINE_float('drop_feat_p_2', 0., 'Probability of node feature dropout 2.')\n",
    "\n",
    "# Logging and checkpoint.\n",
    "flags.DEFINE_string('logdir', None, 'Where the checkpoint and logs are stored.')\n",
    "flags.DEFINE_integer('log_steps', 10, 'Log information at every log_steps.')\n",
    "\n",
    "# Evaluation\n",
    "flags.DEFINE_integer('eval_epochs', 5, 'Evaluate every eval_epochs.')\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    # use CUDA_VISIBLE_DEVICES to select gpu\n",
    "    device = torch.device('cuda:7') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    log.info('Using {} for training.'.format(device))\n",
    "\n",
    "    # set random seed\n",
    "    if FLAGS.model_seed is not None:\n",
    "        log.info('Random seed set to {}.'.format(FLAGS.model_seed))\n",
    "        set_random_seeds(random_seed=FLAGS.model_seed)\n",
    "\n",
    "    # create log directory\n",
    "    # os.makedirs(FLAGS.logdir, exist_ok=True)\n",
    "    # with open(os.path.join(FLAGS.logdir, 'config.cfg'), \"w\") as file:\n",
    "    #     file.write(FLAGS.flags_into_string())  # save config file\n",
    "\n",
    "    # load data\n",
    "    if FLAGS.dataset != 'wiki-cs':\n",
    "        dataset = get_dataset(FLAGS.dataset_dir, FLAGS.dataset)\n",
    "        num_eval_splits = FLAGS.num_eval_splits\n",
    "    else:\n",
    "        dataset, train_masks, val_masks, test_masks = get_wiki_cs(FLAGS.dataset_dir)\n",
    "        num_eval_splits = train_masks.shape[1]\n",
    "\n",
    "    data = dataset[0]  # all dataset include one graph\n",
    "    log.info('Dataset {}, {}.'.format(dataset.__class__.__name__, data))\n",
    "    data = data.to(device)  # permanently move in gpy memory\n",
    "\n",
    "    # prepare transforms\n",
    "    transform_1 = get_graph_drop_transform(drop_edge_p=FLAGS.drop_edge_p_1, drop_feat_p=FLAGS.drop_feat_p_1)\n",
    "    transform_2 = get_graph_drop_transform(drop_edge_p=FLAGS.drop_edge_p_2, drop_feat_p=FLAGS.drop_feat_p_2)\n",
    "\n",
    "    # build networks\n",
    "    input_size, representation_size = data.x.size(1), FLAGS.graph_encoder_layer[-1]\n",
    "    encoder = GCN([input_size] + FLAGS.graph_encoder_layer, batchnorm=True)   # 512, 256, 128\n",
    "    predictor = MLP_Predictor(representation_size, representation_size, hidden_size=FLAGS.predictor_hidden_size)\n",
    "    model = BGRL(encoder, predictor).to(device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = AdamW(model.trainable_parameters(), lr=FLAGS.lr, weight_decay=FLAGS.weight_decay)\n",
    "\n",
    "    # scheduler\n",
    "    lr_scheduler = CosineDecayScheduler(FLAGS.lr, FLAGS.lr_warmup_epochs, FLAGS.epochs)\n",
    "    mm_scheduler = CosineDecayScheduler(1 - FLAGS.mm, 0, FLAGS.epochs)\n",
    "\n",
    "    # setup tensorboard and make custom layout\n",
    "    layout = {'accuracy': {'accuracy/test': ['Multiline', [f'accuracy/test_{i}' for i in range(num_eval_splits)]]}}\n",
    "\n",
    "    def train(step):\n",
    "        model.train()\n",
    "\n",
    "        # update learning rate\n",
    "        lr = lr_scheduler.get(step)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # update momentum\n",
    "        mm = 1 - mm_scheduler.get(step)\n",
    "\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x1, x2 = transform_1(data), transform_2(data)\n",
    "\n",
    "        q1, y2 = model(x1, x2)\n",
    "        q2, y1 = model(x2, x1)\n",
    "\n",
    "        loss = 2 - cosine_similarity(q1, y2.detach(), dim=-1).mean() - cosine_similarity(q2, y1.detach(), dim=-1).mean()\n",
    "        loss.backward()\n",
    "\n",
    "        # update online network\n",
    "        optimizer.step()\n",
    "        # update target network\n",
    "        model.update_target_network(mm)\n",
    "\n",
    "        # log scalars\n",
    "        # print(f\"[Train] Epoch: {step}, Loss: {float(loss):.5f}\")\n",
    "\n",
    "    def eval(epoch):\n",
    "        # make temporary copy of encoder\n",
    "        tmp_encoder = copy.deepcopy(model.online_encoder).eval()\n",
    "        representations, labels = compute_representations(tmp_encoder, dataset, device)\n",
    "\n",
    "        if FLAGS.dataset != 'wiki-cs':\n",
    "            scores = fit_logistic_regression(representations.cpu().numpy(), labels.cpu().numpy(),\n",
    "                                             data_random_seed=FLAGS.data_seed, repeat=FLAGS.num_eval_splits)\n",
    "        else:\n",
    "            scores = fit_logistic_regression_preset_splits(representations.cpu().numpy(), labels.cpu().numpy(),\n",
    "                                                           train_masks, val_masks, test_masks)\n",
    "\n",
    "        for i, score in enumerate(scores):\n",
    "            print(f\"[Test] Epoch: {epoch}, Test Acc: {score:.4f}\")\n",
    "\n",
    "    for epoch in tqdm(range(1, FLAGS.epochs + 1)):\n",
    "        train(epoch-1)\n",
    "        if epoch % FLAGS.eval_epochs == 0:\n",
    "            eval(epoch)\n",
    "\n",
    "    # save encoder weights\n",
    "    # torch.save({'model': model.online_encoder.state_dict()}, os.path.join(FLAGS.logdir, 'bgrl-wikics.pt'))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # print(sys.argv)\n",
    "    # assert len(sys.argv) == 1 \n",
    "    sys.argv = sys.argv[:1]\n",
    "    sys.argv.append('--flagfile=config/coauthor-cs.cfg')\n",
    "    \n",
    "    log.info('PyTorch version: %s' % torch.__version__)\n",
    "    app.run(main)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9361eac9592db6036240c1035ef67375b6423b359c6e68ee240849a077bc4c29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
